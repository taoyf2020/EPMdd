# EPMdd
Enhanced Prompt-Supervised Multimodal Depression Detection Based on Social Media

Nonverbal behavioral data with visual and acoustic features is gaining popularity in depression detection due to its ability to effectively and objectively convey depressive tendencies while reducing the impact of environmental noise. However, neglecting critical auxiliary enhancement descriptors poses challenges in distinguishing between depressed and non-depressed states, especially in low-quality data. To address this issue, this paper proposes an enhanced prompt supervised multimodal depression detection method (EPMdd). Specifically, the proposed probability-based enhancement module (PBEM) leverages a pre-trained large language model (LLM) as a behavior analysis engine to integrate multimodal nonverbal behavioral data with language descriptions during model training. This integration facilitates the robust and fine-grained detection of depressive emotional variations, enhancing the model's feature extraction capability while maintaining computational efficiency. Additionally, we introduce a local-global self-attention module (LGAM) to strengthen the learning of both local and global discriminative features by incorporating temporal dependencies. To further enhance the representation learning of spatial and temporal features, a multimodal cross-attention fusion module (MCAF) is proposed. This module effectively fuses high-level semantic representations of low-level visual and acoustic features.
